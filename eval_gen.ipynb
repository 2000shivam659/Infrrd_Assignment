{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "E-1CvnNDhl1V",
    "outputId": "403efde0-ef4b-4a19-d312-a2389bbd162f"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import joblib\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Load the saved model\n",
    "loaded_model = joblib.load('RF_Model_CV.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "pCZ4vQX2k6wL"
   },
   "outputs": [],
   "source": [
    "# Load the TSV file\n",
    "input_file = 'dataset/val/boxes_transcripts'\n",
    "dir_list = os.listdir(input_file)\n",
    "\n",
    "output_file = 'output_fields'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "MlhuZ7dyhpvA"
   },
   "outputs": [],
   "source": [
    "column_indices = {0: 'OTHER',\n",
    "                  11: 'employerAddressStreet_name',\n",
    "                  13: 'employerName',\n",
    "                  8: 'employeeName',\n",
    "                  4: 'box2FederalIncomeTaxWithheld',\n",
    "                  3: 'box1WagesTipsAndOtherCompensations',\n",
    "                  1: 'box16StateWagesTips',\n",
    "                  2: 'box17StateIncomeTax',\n",
    "                  6: 'box4SocialSecurityTaxWithheld',\n",
    "                  5: 'box3SocialSecurityWages',\n",
    "                  9: 'employerAddressCity',\n",
    "                  12: 'employerAddressZip',\n",
    "                  10: 'employerAddressState',\n",
    "                  15: 'taxYear',\n",
    "                  7: 'einEmployerIdentificationNumber',\n",
    "                  14: 'ssnOfEmployee'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_EMe2tjIhzZZ",
    "outputId": "6b76802a-e115-4cc8-8303-6c8ec4b86145"
   },
   "outputs": [],
   "source": [
    "for file in dir_list:\n",
    "      # Read the input file into a DataFrame\n",
    "      loc = os.path.join(input_file, file)\n",
    "      df = pd.read_csv(loc, sep=',', header=None)\n",
    "\n",
    "      # Extract the first 6 columns as input features\n",
    "      df_input = df.iloc[:, :6]\n",
    "\n",
    "      # Make predictions on the input features\n",
    "      predictions = loaded_model.predict(df_input)\n",
    "\n",
    "      # Add the predictions as a new column to the DataFrame\n",
    "      df['fields'] = predictions\n",
    "\n",
    "      # Map the key to columns value\n",
    "      df['fields'] = df['fields'].map(column_indices)\n",
    "\n",
    "      # Save the DataFrame as a TSV file\n",
    "      output_path = os.path.join(output_file, f'{file}')\n",
    "      df.to_csv(output_path, sep=',', index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hWoC4TxbmnfI"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "t85rywSAqOI-"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Precision  Recall  F1-Score  Support\n",
      "0         1.0     1.0       1.0      157\n",
      "1         1.0     1.0       1.0      182\n",
      "2         1.0     1.0       1.0      347\n",
      "3         1.0     1.0       1.0      370\n",
      "4         1.0     1.0       1.0      337\n",
      "5         1.0     1.0       1.0      772\n",
      "6         1.0     1.0       1.0      288\n",
      "7         1.0     1.0       1.0      193\n",
      "8         1.0     1.0       1.0      363\n",
      "9         1.0     1.0       1.0      164\n",
      "10        1.0     1.0       1.0      638\n",
      "11        1.0     1.0       1.0      311\n",
      "12        1.0     1.0       1.0      186\n",
      "13        1.0     1.0       1.0      300\n",
      "14        1.0     1.0       1.0      282\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import csv\n",
    "import pandas as pd\n",
    "\n",
    "'''\n",
    "Entities:\n",
    "1. employerName\n",
    "2. employerAddressStreet_name\n",
    "3. employerAddressCity\n",
    "4. employerAddressState\n",
    "5. employerAddressZip\n",
    "6. einEmployerIdentificationNumber\n",
    "7. employeeName\n",
    "8. ssnOfEmployee\n",
    "9. box1WagesTipsAndOtherCompensations\n",
    "10. box2FederalIncomeTaxWithheld\n",
    "11. box3SocialSecurityWages\n",
    "12. box4SocialSecurityTaxWithheld\n",
    "13. box16StateWagesTips\n",
    "14. box17StateIncomeTax\n",
    "15. taxYear\n",
    "'''\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "Description: The fuction yields the standard precision, recall and f1 score metrics\n",
    "\n",
    "arguments:\n",
    "    TP -> int\n",
    "    FP -> int\n",
    "    FN -> int\n",
    "\n",
    "returns: float, float, float\n",
    "'''\n",
    "def performance(TP, FP, FN):\n",
    "    \n",
    "    if (TP+FP) == 0:\n",
    "        precision = \"NaN\"\n",
    "    else:\n",
    "        precision = TP/float((TP+FP))\n",
    "        \n",
    "    if (TP+FN) == 0:\n",
    "        recall = \"NaN\"\n",
    "    else:\n",
    "        recall = TP/float((TP+FN))\n",
    "    \n",
    "    if (recall!=\"NaN\") and (precision!=\"NaN\"):\n",
    "        f1_score = (2.0*precision*recall)/(precision+recall)\n",
    "    else:\n",
    "        f1_score = \"NaN\"\n",
    "    \n",
    "    return precision, recall, f1_score\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "'''\n",
    "Description: The fuction yields a dataframe containing entity-wise performance metrics\n",
    "\n",
    "arguments:\n",
    "    true_labels -> list\n",
    "    pred_labels -> lisyt\n",
    "    \n",
    "returns: pandas dataframe\n",
    "'''\n",
    "def get_dataset_metrics(true_labels, pred_labels):\n",
    "    \n",
    "    metrics_dict = dict()\n",
    "    \n",
    "    for true_label, pred_label in zip(true_labels, pred_labels):\n",
    "        if true_label not in metrics_dict:\n",
    "            metrics_dict[true_label] = {\"TP\":0, \"FP\":0, \"FN\":0, \"Support\":0}\n",
    "        \n",
    "        if true_label != \"OTHER\":\n",
    "            metrics_dict[true_label][\"Support\"] += 1\n",
    "            \n",
    "            if true_label == pred_label:\n",
    "                metrics_dict[true_label][\"TP\"] += 1\n",
    "            \n",
    "            elif pred_label == \"OTHER\":\n",
    "                metrics_dict[true_label][\"FN\"] += 1\n",
    "            \n",
    "        else:\n",
    "            if pred_label != \"OTHER\":\n",
    "                metrics_dict[pred_label][\"FP\"] += 1\n",
    "           \n",
    "    df = pd.DataFrame()\n",
    "    \n",
    "    for field in metrics_dict:\n",
    "        precision, recall, f1_score = performance(metrics_dict[field][\"TP\"], metrics_dict[field][\"FP\"], metrics_dict[field][\"FN\"])\n",
    "        support = metrics_dict[field][\"Support\"]\n",
    "        \n",
    "        if field != \"OTHER\":\n",
    "            temp_df = pd.DataFrame([[precision, recall, f1_score, support]], columns=[\"Precision\", \"Recall\", \"F1-Score\", \"Support\"], index=[field])\n",
    "            df = pd.concat([df, temp_df], ignore_index=True)\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "Description: The fuction yields a dataframe containing entity-wise performance metrics for a single document\n",
    "(make sure the doc id is the same)\n",
    "\n",
    "arguments:\n",
    "    doc_true -> tsv file with with labels in the last column (8 th column (1-indexed))\n",
    "    doc_pred -> tsv file with labels in the last column (8 th column (1-indexed)), as predicted by the model\n",
    "    \n",
    "returns: list, list\n",
    "'''\n",
    "def get_doc_labels(doc_true, doc_pred):\n",
    "\n",
    "    true_labels = [row[-1] for row in csv.reader(open(doc_true, \"r\"))]\n",
    "    pred_labels = [row[-1] for row in csv.reader(open(doc_pred, \"r\"))]\n",
    "\n",
    "    return true_labels, pred_labels\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "Description: The fuction yields a dataframe containing entity-wise performance metrics for all documents\n",
    "(make sure the doc ids are the same in both the paths)\n",
    "\n",
    "arguments:\n",
    "    doc_true -> string (directory containing the ground truth tsv files)\n",
    "    doc_pred -> string (directory containing the predicted tsv files)\n",
    "    save -> bool (saves the metrics file in your working directory)\n",
    "returns: pandas dataframe\n",
    "'''\n",
    "def get_dataset_labels(true_path, pred_path, save=False):\n",
    "    \n",
    "    y_true, y_pred = [], []\n",
    "    \n",
    "    for true_file in os.listdir(true_path):\n",
    "        for pred_file in os.listdir(pred_path):\n",
    "            if (\".tsv\" in true_file) and (\".tsv\" in pred_file):\n",
    "                if true_file == pred_file:\n",
    "                    \n",
    "                    true_file, pred_file = f\"{true_path}/{true_file}\", f\"{pred_path}/{pred_file}\"\n",
    "                    true_labels, pred_labels = get_doc_labels(true_file, pred_file)\n",
    "                    \n",
    "                    y_true.extend(true_labels)\n",
    "                    y_pred.extend(pred_labels)\n",
    "            \n",
    "    df = get_dataset_metrics(y_true, y_pred)\n",
    "    print(df)\n",
    "    if save == True:\n",
    "        df.to_csv('eval_metrics.tsv', sep=',', index=False, header=False)\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    # template to run your own evaluation\n",
    "\n",
    "    doc_true = f\"{os.getcwd()}/output_fields\"\n",
    "    doc_pred = f\"{os.getcwd()}/output_fields\"\n",
    "\n",
    "    get_dataset_labels(doc_true, doc_pred, save=True)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
